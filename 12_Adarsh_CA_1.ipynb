{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tSGdo-PlpAn",
        "outputId": "8a2da428-8a46-4ab8-ac1d-9ac5e44a33d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
            "Training shape: (50000, 32, 32, 3)\n",
            "Testing shape: (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 161ms/step - accuracy: 0.3131 - loss: 1.8628 - val_accuracy: 0.5012 - val_loss: 1.3805\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - accuracy: 0.5130 - loss: 1.3513 - val_accuracy: 0.5460 - val_loss: 1.2677\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 159ms/step - accuracy: 0.5738 - loss: 1.1951 - val_accuracy: 0.6011 - val_loss: 1.1311\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 159ms/step - accuracy: 0.6127 - loss: 1.0926 - val_accuracy: 0.6258 - val_loss: 1.0466\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 158ms/step - accuracy: 0.6461 - loss: 1.0002 - val_accuracy: 0.6506 - val_loss: 0.9948\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 150ms/step - accuracy: 0.6660 - loss: 0.9407 - val_accuracy: 0.6697 - val_loss: 0.9482\n",
            "Epoch 7/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 164ms/step - accuracy: 0.6885 - loss: 0.8884 - val_accuracy: 0.6737 - val_loss: 0.9367\n",
            "Epoch 8/10\n",
            "\u001b[1m163/391\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6983 - loss: 0.8505"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import required modules from Keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# Import matplotlib (used for plotting graphs if needed)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "# It contains 60,000 color images of size 32x32 in 10 classes\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "# Normalize pixel values from range (0-255) to (0-1)\n",
        "# This improves training performance\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training shape:\", x_train.shape)   # (50000, 32, 32, 3)\n",
        "print(\"Testing shape:\", x_test.shape)     # (10000, 32, 32, 3)\n",
        "\n",
        "\n",
        "# Create Sequential model (layers added one after another)\n",
        "model = models.Sequential()\n",
        "\n",
        "\n",
        "# First Convolution Layer\n",
        "# 32 filters of size 3x3\n",
        "# ReLU activation\n",
        "# Input shape is 32x32 with 3 color channels (RGB)\n",
        "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "\n",
        "# MaxPooling layer reduces spatial size by 2x2\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "\n",
        "# Second Convolution Layer\n",
        "# 64 filters\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "# MaxPooling again\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "\n",
        "# Third Convolution Layer\n",
        "# 64 filters\n",
        "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
        "\n",
        "\n",
        "# Flatten 3D feature maps into 1D vector\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "\n",
        "# Fully connected dense layer\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "# 10 neurons (for 10 classes)\n",
        "# No activation because we use logits\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',   # Adaptive learning optimizer\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,              # Train for 10 iterations\n",
        "    batch_size=128,         # Process 128 images at once\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_acc)"
      ]
    }
  ]
}